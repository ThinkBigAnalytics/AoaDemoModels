{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Once we have finished experiementation and found a good model, we want to operationalize it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usernamewf250003\n",
      "password········\n"
     ]
    }
   ],
   "source": [
    "from teradataml import create_context\n",
    "import getpass\n",
    "\n",
    "username = input(\"Username\")\n",
    "password = getpass.getpass(\"password\")\n",
    "\n",
    "# by default we assume your are using your user database. change as required\n",
    "database = username\n",
    "\n",
    "engine = create_context(host=\"3.238.151.85\", username=username, password=password, logmech=\"TDNEGO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Function\n",
    "\n",
    "The training function takes the following shape\n",
    "\n",
    "```python\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # your training code\n",
    "    \n",
    "    # save your model\n",
    "    joblib.dump(model, f\"{context.artifact_output_path}/model.joblib\")\n",
    "    \n",
    "    record_training_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../model_modules/training.py\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nyoka import xgboost_to_pmml\n",
    "from teradataml import DataFrame\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "\n",
    "    # read training dataset from Teradata and convert to pandas\n",
    "    train_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    train_pdf = train_df.to_pandas(all_rows=True)\n",
    "\n",
    "    # split data into X and y\n",
    "    X_train = train_pdf[feature_names]\n",
    "    y_train = train_pdf[target_name]\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # fit model to training data\n",
    "    model = Pipeline([('scaler', MinMaxScaler()),\n",
    "                      ('xgb', XGBClassifier(eta=context.hyperparams[\"eta\"],\n",
    "                                            max_depth=context.hyperparams[\"max_depth\"]))])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # export model artefacts\n",
    "    joblib.dump(model, f\"{context.artifact_output_path}/model.joblib\")\n",
    "\n",
    "    # we can also save as pmml so it can be used for In-Vantage scoring etc.\n",
    "    xgboost_to_pmml(pipeline=model, col_names=feature_names, target_name=target_name,\n",
    "                    pmml_f_name=f\"{context.artifact_output_path}/model.pmml\")\n",
    "\n",
    "    print(\"Saved trained model\")\n",
    "\n",
    "    from xgboost import plot_importance\n",
    "    model[\"xgb\"].get_booster().feature_names = feature_names\n",
    "    plot_importance(model[\"xgb\"].get_booster(), max_num_features=10)\n",
    "    save_plot(\"feature_importance.png\", context=context)\n",
    "\n",
    "    feature_importance = model[\"xgb\"].get_booster().get_score(importance_type=\"weight\")\n",
    "\n",
    "    record_training_stats(train_df,\n",
    "                          features=feature_names,\n",
    "                          predictors=[target_name],\n",
    "                          categorical=[target_name],\n",
    "                          importance=feature_importance,\n",
    "                          context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:aoa.util.connections:teradataml context already exists. Skipping create_context.\n",
      "Starting training...\n",
      "Finished training\n",
      "Saved trained model\n",
      "INFO:aoa.stats.stats:Computing training dataset statistics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the training dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*, D.hasdiabetes\n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "JOIN PIMA_PATIENT_DIAGNOSES D\n",
    "ON F.patientid = D.patientid\n",
    "    WHERE D.patientid MOD 5 <> 0\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": database,\n",
    "    \"table\": \"aoa_feature_metadata\"\n",
    "}\n",
    "hyperparams = {\"max_depth\": 5, \"eta\": 0.2}\n",
    "\n",
    "entity_key = \"PatientId\"\n",
    "target_names = [\"HasDiabetes\"]\n",
    "feature_names = [\"NumTimesPrg\", \"PlGlcConc\", \"BloodP\", \"SkinThick\", \"TwoHourSerIns\", \"BMI\", \"DiPedFunc\", \"Age\"]\n",
    " \n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"/tmp\",\n",
    "                   model_version=\"v1\",\n",
    "                   model_table=\"aoa_model_v1\")\n",
    "\n",
    "train(context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Function\n",
    "\n",
    "The evaluation function takes the following shape\n",
    "\n",
    "```python\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    model = joblib.load(f\"{context.artifact_input_path}/model.joblib\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_evaluation_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../model_modules/evaluation.py\n",
    "\n",
    "from sklearn import metrics\n",
    "from teradataml import DataFrame, copy_to_sql\n",
    "from aoa import (\n",
    "    record_evaluation_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    model = joblib.load(f\"{context.artifact_input_path}/model.joblib\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    test_pdf = test_df.to_pandas(all_rows=True)\n",
    "\n",
    "    X_test = test_pdf[feature_names]\n",
    "    y_test = test_pdf[target_name]\n",
    "\n",
    "    print(\"Scoring\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred_tdf = pd.DataFrame(y_pred, columns=[target_name])\n",
    "    y_pred_tdf[\"PatientId\"] = test_pdf[\"PatientId\"].values\n",
    "\n",
    "    evaluation = {\n",
    "        'Accuracy': '{:.2f}'.format(metrics.accuracy_score(y_test, y_pred)),\n",
    "        'Recall': '{:.2f}'.format(metrics.recall_score(y_test, y_pred)),\n",
    "        'Precision': '{:.2f}'.format(metrics.precision_score(y_test, y_pred)),\n",
    "        'f1-score': '{:.2f}'.format(metrics.f1_score(y_test, y_pred))\n",
    "    }\n",
    "\n",
    "    with open(f\"{context.artifact_output_path}/metrics.json\", \"w+\") as f:\n",
    "        json.dump(evaluation, f)\n",
    "\n",
    "    metrics.plot_confusion_matrix(model, X_test, y_test)\n",
    "    save_plot('Confusion Matrix', context=context)\n",
    "\n",
    "    metrics.plot_roc_curve(model, X_test, y_test)\n",
    "    save_plot('ROC Curve', context=context)\n",
    "\n",
    "    # xgboost has its own feature importance plot support but lets use shap as explainability example\n",
    "    import shap\n",
    "\n",
    "    shap_explainer = shap.TreeExplainer(model['xgb'])\n",
    "    shap_values = shap_explainer.shap_values(X_test)\n",
    "\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names,\n",
    "                      show=False, plot_size=(12, 8), plot_type='bar')\n",
    "    save_plot('SHAP Feature Importance', context=context)\n",
    "\n",
    "    feature_importance = pd.DataFrame(list(zip(feature_names, np.abs(shap_values).mean(0))),\n",
    "                                      columns=['col_name', 'feature_importance_vals'])\n",
    "    feature_importance = feature_importance.set_index(\"col_name\").T.to_dict(orient='records')[0]\n",
    "\n",
    "    predictions_table = \"evaluation_preds_tmp\"\n",
    "    copy_to_sql(df=y_pred_tdf, table_name=predictions_table, index=False, if_exists=\"replace\", temporary=True)\n",
    "\n",
    "    record_evaluation_stats(features_df=test_df,\n",
    "                            predicted_df=DataFrame(predictions_table),\n",
    "                            importance=feature_importance,\n",
    "                            context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:aoa.util.connections:teradataml context already exists. Skipping create_context.\n",
      "Scoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:aoa.stats.stats:Computing evaluation dataset statistics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the evaluation dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*, D.hasdiabetes \n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "JOIN PIMA_PATIENT_DIAGNOSES D\n",
    "ON F.patientid = D.patientid\n",
    "    WHERE D.patientid MOD 5 = 0\n",
    "\"\"\"\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"/tmp\",\n",
    "                   artifact_input_path=\"/tmp\",\n",
    "                   model_version=\"v1\",\n",
    "                   model_table=\"aoa_model_v1\")\n",
    "\n",
    "evaluate(context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Scoring Function\n",
    "\n",
    "The scoring function takes the following shape\n",
    "\n",
    "```python\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    model = joblib.load(f\"{context.artifact_input_path}/model.joblib\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_scoring_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../model_modules/scoring.py\n",
    "\n",
    "from teradataml import copy_to_sql, DataFrame\n",
    "from aoa import (\n",
    "    record_scoring_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def score(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    model = joblib.load(f\"{context.artifact_input_path}/model.joblib\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    features_tdf = DataFrame.from_query(context.dataset_info.sql)\n",
    "    features_pdf = features_tdf.to_pandas(all_rows=True)\n",
    "\n",
    "    print(\"Scoring\")\n",
    "    predictions_pdf = model.predict(features_pdf[feature_names])\n",
    "\n",
    "    print(\"Finished Scoring\")\n",
    "\n",
    "    # store the predictions\n",
    "    predictions_pdf = pd.DataFrame(predictions_pdf, columns=[target_name])\n",
    "    predictions_pdf[entity_key] = features_pdf.index.values\n",
    "    # add job_id column so we know which execution this is from if appended to predictions table\n",
    "    predictions_pdf[\"job_id\"] = context.job_id\n",
    "\n",
    "    # teradataml doesn't match column names on append.. and so to match / use same table schema as for byom predict\n",
    "    # example (see README.md), we must add empty json_report column and change column order manually (v17.0.0.4)\n",
    "    # CREATE MULTISET TABLE pima_patient_predictions\n",
    "    # (\n",
    "    #     job_id VARCHAR(255), -- comes from airflow on job execution\n",
    "    #     PatientId BIGINT,    -- entity key as it is in the source data\n",
    "    #     HasDiabetes BIGINT,   -- if model automatically extracts target\n",
    "    #     json_report CLOB(1048544000) CHARACTER SET UNICODE  -- output of\n",
    "    # )\n",
    "    # PRIMARY INDEX ( job_id );\n",
    "    predictions_pdf[\"json_report\"] = \"\"\n",
    "    predictions_pdf = predictions_pdf[[\"job_id\", entity_key, target_name, \"json_report\"]]\n",
    "\n",
    "    copy_to_sql(df=predictions_pdf,\n",
    "                schema_name=context.dataset_info.predictions_database,\n",
    "                table_name=context.dataset_info.predictions_table,\n",
    "                index=False,\n",
    "                if_exists=\"append\")\n",
    "    \n",
    "    print(\"Saved predictions in Teradata\")\n",
    "\n",
    "    # calculate stats\n",
    "    predictions_df = DataFrame.from_query(f\"\"\"\n",
    "        SELECT \n",
    "            * \n",
    "        FROM {context.dataset_info.get_predictions_metadata_fqtn()} \n",
    "            WHERE job_id = '{context.job_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    record_scoring_stats(features_df=features_tdf, predicted_df=predictions_df, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:aoa.util.connections:teradataml context already exists. Skipping create_context.\n",
      "Scoring\n",
      "Finished Scoring\n",
      "Saved predictions in Teradata\n",
      "INFO:aoa.stats.stats:Computing scoring dataset statistics\n",
      "WARNING:aoa.stats.metrics:Publishing scoring metrics is not enabled\n"
     ]
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the scoring dataset \n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*\n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "    WHERE F.patientid MOD 5 = 0\n",
    "\"\"\"\n",
    "\n",
    "# where to store predictions\n",
    "predictions = {\n",
    "    \"database\": database,\n",
    "    \"table\": \"pima_patient_predictions_tmp\"\n",
    "}\n",
    "\n",
    "import uuid\n",
    "job_id=str(uuid.uuid4())\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata,\n",
    "                           predictions=predictions)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"/tmp\",\n",
    "                   artifact_input_path=\"/tmp\",\n",
    "                   model_version=\"v1\",\n",
    "                   model_table=\"aoa_model_v1\",\n",
    "                   job_id=job_id)\n",
    "\n",
    "score(context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Metadata\n",
    "\n",
    "Finally, create the configuration files.\n",
    "\n",
    "Requirements file with the dependencies and versions\n",
    "\n",
    "```\n",
    "%%writefile ../model_modules/requirements.txt\n",
    "xgboost==0.90\n",
    "scikit-learn==0.24.2\n",
    "shap==0.36.0\n",
    "matplotlib==3.3.1\n",
    "teradataml==17.0.0.4\n",
    "nyoka==4.3.0\n",
    "aoa==6.0.0\n",
    "```\n",
    "\n",
    "The hyper parameter configuration (defaults)\n",
    "```\n",
    "%%writefile ../config.json\n",
    "{\n",
    "   \"hyperParameters\": {\n",
    "      \"eta\": 0.2,\n",
    "      \"max_depth\": 6\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aoa] *",
   "language": "python",
   "name": "conda-env-aoa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
